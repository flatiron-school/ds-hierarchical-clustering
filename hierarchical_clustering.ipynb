{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Objectives\" data-toc-modified-id=\"Objectives-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Objectives</a></span></li><li><span><a href=\"#Why-$k$-means-Isn't-Enough\" data-toc-modified-id=\"Why-$k$-means-Isn't-Enough-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Why $k$-means Isn't Enough</a></span></li><li><span><a href=\"#Hierarchical-Agglomerative-Clustering\" data-toc-modified-id=\"Hierarchical-Agglomerative-Clustering-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Hierarchical Agglomerative Clustering</a></span><ul class=\"toc-item\"><li><span><a href=\"#How-Does-the-Algorithm-Work\" data-toc-modified-id=\"How-Does-the-Algorithm-Work-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>How Does the Algorithm Work</a></span><ul class=\"toc-item\"><li><span><a href=\"#Agglomerative-Example\" data-toc-modified-id=\"Agglomerative-Example-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>Agglomerative Example</a></span></li></ul></li><li><span><a href=\"#Visualizing-with-Dendrograms\" data-toc-modified-id=\"Visualizing-with-Dendrograms-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Visualizing with Dendrograms</a></span><ul class=\"toc-item\"><li><span><a href=\"#Advantages-to-Using-Dendrograms\" data-toc-modified-id=\"Advantages-to-Using-Dendrograms-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Advantages to Using Dendrograms</a></span></li><li><span><a href=\"#ASIDE:-How-Well-Does-the-Dendrogram-Fit-the-Data?\" data-toc-modified-id=\"ASIDE:-How-Well-Does-the-Dendrogram-Fit-the-Data?-3.2.2\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;</span>ASIDE: How Well Does the Dendrogram Fit the Data?</a></span></li></ul></li></ul></li><li><span><a href=\"#Types-of-Hierarchical-Agglomerative-Clustering\" data-toc-modified-id=\"Types-of-Hierarchical-Agglomerative-Clustering-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Types of Hierarchical Agglomerative Clustering</a></span></li><li><span><a href=\"#Seeing-it-in-action\" data-toc-modified-id=\"Seeing-it-in-action-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Seeing it in action</a></span><ul class=\"toc-item\"><li><span><a href=\"#Hierarchical-clustering-with-scipy\" data-toc-modified-id=\"Hierarchical-clustering-with-scipy-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Hierarchical clustering with <code>scipy</code></a></span></li><li><span><a href=\"#Hierarchical-clustering-with-sklearn-on-Iris\" data-toc-modified-id=\"Hierarchical-clustering-with-sklearn-on-Iris-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Hierarchical clustering with <code>sklearn</code> on Iris</a></span></li><li><span><a href=\"#Evaluate\" data-toc-modified-id=\"Evaluate-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Evaluate</a></span><ul class=\"toc-item\"><li><span><a href=\"#Evaluating-number-of-clusters-/-Cut-points\" data-toc-modified-id=\"Evaluating-number-of-clusters-/-Cut-points-5.3.1\"><span class=\"toc-item-num\">5.3.1&nbsp;&nbsp;</span>Evaluating number of clusters / Cut points</a></span></li></ul></li></ul></li><li><span><a href=\"#Advantages-&amp;-Disadvantages-of-hierarchical-clustering\" data-toc-modified-id=\"Advantages-&amp;-Disadvantages-of-hierarchical-clustering-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Advantages &amp; Disadvantages of hierarchical clustering</a></span><ul class=\"toc-item\"><li><span><a href=\"#Advantages\" data-toc-modified-id=\"Advantages-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Advantages</a></span></li><li><span><a href=\"#Disadvantages\" data-toc-modified-id=\"Disadvantages-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Disadvantages</a></span><ul class=\"toc-item\"><li><span><a href=\"#Further-reading\" data-toc-modified-id=\"Further-reading-6.2.1\"><span class=\"toc-item-num\">6.2.1&nbsp;&nbsp;</span>Further reading</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "ex_path = os.path.abspath(os.pardir)\n",
    "if ex_path not in sys.path:\n",
    "    sys.path.append(ex_path)\n",
    "\n",
    "from src.hier_example import *\n",
    "from sklearn.datasets import make_blobs\n",
    "from src.av_link_agglom_clust import centrAggClust as ac\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, cophenet\n",
    "from scipy.spatial.distance import pdist\n",
    "from sklearn.datasets import make_blobs, make_moons, load_iris\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Describe the algorithm for agglomerative hierarchical clustering\n",
    "- Compare and contrast hierarchical clustering with $k$-means clustering\n",
    "- Implement hierarchical clustering with `scipy` and `sklearn`\n",
    "- Build and interpret dendrograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why $k$-means Isn't Enough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Recall that $k$-means clustering assigns individual observations to a _pre-specified number of clusters_ according to the distance between the centroid and the observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One disadvantage of $k$-means clustering is that we have to specify the number of clusters beforehand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering problems are sometimes difficult to say when you got it \"right\" since technically, there isn't necessarily a right answer. It really depends on the _context_ of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn shows some examples of different clustering algorithms on different datasets in its [documentation](https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html): \n",
    "![](https://scikit-learn.org/stable/_images/sphx_glr_plot_cluster_comparison_001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hierarchical Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "> Hierarchical agglomerative clustering sets out to group the most similar two observations together from a bottom-up level. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The type of hierarchical clustering we will learn today is **agglomerative**, or **bottom-up**. We don't have to specify the number of clusters beforehand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How Does the Algorithm Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with some points that we want to cluster (ignore the colors for now):\n",
    "\n",
    "<img src=\"images/pre_cluster_points.png\" width=30%/>\n",
    "\n",
    "> Picture what you would picture to be reasonable groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- In the algorithm, initially every observation is its own cluster.\n",
    "\n",
    "- Then we group points that are _most similar (nearest)_ to each other as a cluster.\n",
    "\n",
    "- As we get small clusters, we group already formed clusters with nearby points and other clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ultimately creates one large cluster but with smaller clusters making up the larger clusters. Each \"smaller\" cluster's members are overall more like each other than with the larger clusters.\n",
    "\n",
    "> We observe \"clusters within clusters\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/yo_dawg_clusters_with_clusters.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Results from Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|BEFORE | AFTER |\n",
    "|:-----:| :---: |\n",
    "|<img src=\"images/pre_cluster_points.png\" width=60%/> | <img src=\"images/clustergram.png\" width=100%/> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Agglomerative Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_agglomerative_algorithm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing with Dendrograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize the clustering with a tree-like diagram called a **dendrogram**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Clustering | Dendrograms |\n",
    "|:-----:| :---: |\n",
    "|<img src=\"images/clustergram.png\" width=100%/> | <img src=\"images/dendrogram.png\" width=100%/> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows us to view the clusterings obtained for each possible number of clusters, from 1 to n. It is up to our discretion as data scientists to decide how many clusters we want. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As we move up the leaf of the dendrogram, the two most similar observations fuse together, and then the next most similar clusters fuse together etc. until everything fuses together into a big cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages to Using Dendrograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Useful in visualizing HIGH dimensional separation\n",
    "- Can choose just \"n\" clusters\n",
    "    + cut the link of the highest tree for two clusters\n",
    "    + clusters are more alike"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ASIDE: How Well Does the Dendrogram Fit the Data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "One way of computing this is by means of the **cophenetic correlation coefficient**, which, in a word, is a measure of \"how faithfully the dendrogram preserves the pairwise distances between the \\[datapoints\\]\" -- [Wikipedia](https://en.wikipedia.org/wiki/Cophenetic_correlation).\n",
    "\n",
    "The cophenetic correlation coefficient $c$ is given by [ref](https://www.mathworks.com/help/stats/index.html?/access/helpdesk/help/toolbox/stats/cophenet.html=)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![c-coef](images/cophenet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$x(i, j) = | Xi − Xj |$, the ordinary Euclidean distance between the $i$th and $j$th observations.<br>\n",
    "$t(i, j)$ = the dendrogrammatic distance between the model points $Ti$ and $Tj$. This distance is the height of the node at which these two points are first joined together.<br>\n",
    "\n",
    "Then, letting ${\\bar {x}}$ be the average of the $x(i, j)$, and letting ${\\bar {t}}$ be the average of the $t(i, j)$, the cophenetic correlation coefficient $c$ is given by[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is complicated! [This site](https://people.revoledu.com/kardi/tutorial/Clustering/Online-Hierarchical-Clustering.html) is helpful on how to understand the cophenetic correlation, and, indeed, on hierarchical clustering generally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Types of Hierarchical Agglomerative Clustering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The way that distance is measured between clusters is called the model's **linkage**.\n",
    "\n",
    "- Single Linkage \n",
    "    -  Minimum pair-wise distance: for any two clusters, take one observation from each and determine their distance. Do this over and over, until you have identified the overall minimum pair-wise distance. \n",
    "- Complete Linkage\n",
    "    -  Complete linkage may be defined as the furthest (or maximum) distance between two clusters. That is, all possible pairwise distances between elements (one from cluster A and one from B) are evaluated and the largest value is used as the distance between clusters A & B. This is sometimes called complete linkage and is also called furthest neighbor.\n",
    "- Average Linkage\n",
    "    - The distance between clusters is defined as the average distance between the data points in the clusters. \n",
    "- Ward Linkage\n",
    "    -  Ward method finds the pair of clusters that leads to minimum increase in total within-cluster variance after merging at each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[This article](https://towardsdatascience.com/understanding-the-concept-of-hierarchical-clustering-technique-c6e8243758ec) describes the pros and cons of each approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Seeing it in action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[This post here](https://www.analyticsvidhya.com/blog/2019/05/beginners-guide-hierarchical-clustering/) walks through cluster assignment _step_ by _step_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X, Y = make_blobs(n_samples=10, n_features=1,\n",
    "                  centers=5, random_state=42)\n",
    "ac(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = [1, 2, 5, 7, 9]\n",
    "Y2 = [2, 2, 8, 2, 2]\n",
    "\n",
    "ac(X2, Y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Meanwhile, we can do it in _**scipy**_ and _**sklearn**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hierarchical clustering with `scipy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Let's generate some data and look at an example of\n",
    "# hierarchical agglomerative clustering.\n",
    "# Generate two clusters: a with 100 points, b with 50:\n",
    "\n",
    "np.random.seed(1000)\n",
    "\n",
    "a = np.random.multivariate_normal([10, 0], [[3, 1], [1, 4]], size=[100,])\n",
    "b = np.random.multivariate_normal([0, 20], [[3, 1], [1, 4]], size=[50,])\n",
    "X = np.concatenate((a, b))\n",
    "\n",
    "print(X.shape)  # 150 samples with 2 dimensions\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[:, 0], X[:, 1])\n",
    "ax.set_title(\"Sample data for clustering demo\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# construct dendrogram in scipy\n",
    "\n",
    "Z = linkage(X, 'single')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Z[:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z contains the dendrogram in coded form! Its third column has distances. https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pdist()` calculates pairwise distances for an input array of points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:3, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(((X[0, 0] - X[1, 0])**2 + (X[0, 1] - X[1, 1])**2)**0.5)\n",
    "print(((X[0, 0] - X[2, 0])**2 + (X[0, 1] - X[2, 1])**2)**0.5)\n",
    "print(((X[0, 0] - X[3, 0])**2 + (X[0, 1] - X[3, 1])**2)**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdist(X)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pdist(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each point, there are 149 distances\n",
    "# to other points.\n",
    "\n",
    "150*149 // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c, coph_dists = cophenet(Z, pdist(X))\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coph_dists[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# calculate and construct the dendrogram \n",
    "# calculate full dendrogram\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(25, 10))\n",
    "ax.set_title('Hierarchical Clustering Dendrogram')\n",
    "ax.set_xlabel('sample index')\n",
    "ax.set_ylabel('distance')\n",
    "dendrogram(\n",
    "    Z,\n",
    "    leaf_rotation=90.,  # rotates the x axis labels\n",
    "    leaf_font_size=8.,  # font size for the x axis labels\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# trimming and truncating the dendrogram \n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_title('Hierarchical Clustering Dendrogram (truncated)')\n",
    "ax.set_xlabel('sample index')\n",
    "ax.set_ylabel('distance')\n",
    "dendrogram(\n",
    "    Z,\n",
    "    truncate_mode='lastp',  # show only the last p merged clusters\n",
    "    p=6,\n",
    "    show_leaf_counts=False, # otherwise numbers in brackets are counts\n",
    "    leaf_rotation=90.,\n",
    "    leaf_font_size=12.,\n",
    "    show_contracted=True    # to get a distribution impression in\n",
    "                            #truncated branches\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hierarchical clustering with `sklearn` on Iris\n",
    "\n",
    "**[Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html)** for AgglomerativeClustering in `sklearn`\n",
    "\n",
    "\n",
    "**[A great example of using Manhattan distance](https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_clustering_metrics.html#sphx-glr-auto-examples-cluster-plot-agglomerative-clustering-metrics-py)** with agglomerative clustering in `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also use the scikitlearn module hierarchical clustering\n",
    "# to perform the same task\n",
    "\n",
    "np.random.seed(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# try clustering on the iris dataset\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "# in this case, we won't be working with predicting labels,\n",
    "# so we will only use the features (X)\n",
    "\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X_iris[:, 1], X_iris[:, 3], c=y_iris);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Comparing our clusters with truth. Note that we are fitting our\n",
    "# model to ALL the columns but only plotting two of them!\n",
    "\n",
    "iris_cluster = AgglomerativeClustering(n_clusters=3)\n",
    "iris_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_iris_clust = iris_cluster.fit_predict(X_iris)\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].scatter(X_iris[:, 1], X_iris[:, 3],\n",
    "             c=y_iris)\n",
    "ax[1].scatter(X_iris[:, 1], X_iris[:, 3],\n",
    "           c=pred_iris_clust);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch(x):\n",
    "    if x == 1:\n",
    "        return 0\n",
    "    elif x == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pred_iris_clust = iris_cluster.fit_predict(X_iris)\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].scatter(X_iris[:, 1], X_iris[:, 3],\n",
    "             c=y_iris)\n",
    "ax[1].scatter(X_iris[:, 1], X_iris[:, 3],\n",
    "           c=list(map(switch, pred_iris_clust)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To evaluate you might try different numbers of clusters and compare their silhouette score as you did with $k$-means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation: silhouette score\n",
    "\n",
    "silhouette_score(X_iris, pred_iris_clust)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Evaluating number of clusters / Cut points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For hierarchical agglomerative clustering, or clustering in general, it is generally difficult to truly evaluate the results. Therefore, it is up you, the data scientists, to decide.\n",
    "\n",
    "**[Stanford has a good explanation on page 380](https://nlp.stanford.edu/IR-book/pdf/17hier.pdf)** of your options for picking the cut-off. \n",
    "\n",
    "When we are viewing dendrograms for hierarchical agglomerative clustering, we can visually examine where the natural cutoff is, despite it not sounding exactly statistical, or scientific. We might want to interpret the clusters and assign meanings to them depending on domain-specific knowledge and shape of dendrogram. However, we can evaluate the quality of our clusters using measurements such as Sihouette score discussed in the k-means lectures. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Advantages & Disadvantages of hierarchical clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Advantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Intuitive and easy to implement\n",
    "- More informative than k-means because it takes individual relationship into consideration\n",
    "- Allows us to look at dendrogram and decide number of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Disadvantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Very sensitive to outliers\n",
    "- Cannot undo the previous merge, which might lead to problems later on "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Further reading\n",
    "\n",
    "- [from MIT on just hierarchical](http://web.mit.edu/6.S097/www/resources/Hierarchical.pdf)\n",
    "- [from MIT comparing clustering methods](http://www.mit.edu/~9.54/fall14/slides/Class13.pdf)\n",
    "- [fun CMU slides on clustering](http://www.cs.cmu.edu/afs/andrew/course/15/381-f08/www/lectures/clustering.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "TOC",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "261px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
